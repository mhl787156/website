---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Towards an Embodied Semantic Fovea: Semantic 3D scene reconstruction from
  ego-centric eye-tracker videos'
subtitle: ''
summary: ''
authors:
- Mickey Li
- Noyan Songur
- Pavel Orlov
- Stefan Leutenegger
- A Aldo Faisal
tags: []
categories: []
date: '2018-07-01'
lastmod: 2022-09-17T11:19:53+01:00
featured: false
draft: false
url_pdf: https://arxiv.org/pdf/1807.10561.pdf

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-09-17T10:24:19.842097Z'
publication_types:
- '2'
abstract: Incorporating the physical environment is essential for a complete understanding
  of human behavior in unconstrained every-day tasks. This is especially important
  in ego-centric tasks where obtaining 3 dimensional information is both limiting
  and challenging with the current 2D video analysis methods proving insufficient.
  Here we demonstrate a proof-of-concept system which provides real-time 3D mapping
  and semantic labeling of the local environment from an ego-centric RGB-D video-stream
  with 3D gaze point estimation from head mounted eye tracking glasses. We augment
  existing work in Semantic Simultaneous Localization And Mapping (Semantic SLAM)
  with collected gaze vectors. Our system can then find and track objects both inside
  and outside the user field-of-view in 3D from multiple perspectives with reasonable
  accuracy. We validate our concept by producing a semantic map from images of the
  NYUv2 dataset while simultaneously estimating gaze position and gaze classes from
  recorded gaze data of the dataset images.
publication: '*European Conference on Computer Vision 2018 Workshop on Egocentric Perception and Computing*'
links:
- name: URL
  url: https://www.eyewear-computing.org/EPIC_ECCV18/program.html
---
